{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (838, 25) (838,)\n",
      "Test set: (280, 25) (280,)\n",
      "Validation set: (280, 25) (280,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gen import generation\n",
    "\n",
    "excel_file = 'PID&SI.xlsx'\n",
    "pd_data = pd.read_excel(excel_file, sheet_name='PDs')\n",
    "suicide_data = pd.read_excel(excel_file, sheet_name='Suicide')\n",
    "\n",
    "assert pd_data['serial'].equals(suicide_data['serial'])\n",
    "\n",
    "X = pd_data.iloc[:, 1:26]\n",
    "y = suicide_data['SI'] \n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Train set:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set:\", X_test.shape, y_test.shape)\n",
    "print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "\n",
    "class SuicideRiskMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SuicideRiskMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(25, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 14)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "    \n",
    "def train(model, optimizer, criterion, sample):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(sample):\n",
    "        optimizer.zero_grad()\n",
    "        input = X_train_tensor[i]\n",
    "        label = y_train_tensor[i]\n",
    "        params = model(input)\n",
    "        pred_result = generation(params).squeeze(-1)\n",
    "        loss = criterion(pred_result, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  91.73946711457393\n",
      "epoch:  1  loss:  37.76447095256299\n",
      "epoch:  2  loss:  34.77809787914157\n",
      "epoch:  3  loss:  32.69761935714632\n",
      "epoch:  4  loss:  32.70580733008683\n",
      "epoch:  5  loss:  30.41028437949717\n",
      "epoch:  6  loss:  31.07433419395238\n",
      "epoch:  7  loss:  29.016595857217908\n",
      "epoch:  8  loss:  29.59547765366733\n",
      "epoch:  9  loss:  28.034149248618633\n",
      "epoch:  10  loss:  30.812334273941815\n",
      "epoch:  11  loss:  26.709225160069764\n",
      "epoch:  12  loss:  27.421477164374664\n",
      "epoch:  13  loss:  23.03218270186335\n",
      "epoch:  14  loss:  21.595563870389014\n",
      "epoch:  15  loss:  21.526283767423593\n",
      "epoch:  16  loss:  20.69553782074945\n",
      "epoch:  17  loss:  20.45206642683479\n",
      "epoch:  18  loss:  18.129262479720637\n",
      "epoch:  19  loss:  17.0380091286097\n",
      "epoch:  20  loss:  18.42891854827394\n",
      "epoch:  21  loss:  13.881704829633236\n",
      "epoch:  22  loss:  10.297905685789374\n",
      "epoch:  23  loss:  8.750003327304853\n",
      "epoch:  24  loss:  18.540874932018596\n",
      "epoch:  25  loss:  8.187627448522107\n",
      "epoch:  26  loss:  7.66870599096228\n",
      "epoch:  27  loss:  19.514269306714\n",
      "epoch:  28  loss:  21.42506805917219\n",
      "epoch:  29  loss:  7.3981040935422016\n"
     ]
    }
   ],
   "source": [
    "model1 = SuicideRiskMLP()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model1.parameters(), lr=0.001)\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model1.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(50):\n",
    "        optimizer.zero_grad()\n",
    "        input = X_train_tensor[i]\n",
    "        label = y_train_tensor[i]\n",
    "        params = model1(input)\n",
    "        pred_result = generation(params).squeeze(-1)\n",
    "        loss = criterion(pred_result, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(\"epoch: \", epoch, \" loss: \", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  628.1432833584636\n",
      "epoch:  1  loss:  487.95516322390176\n",
      "epoch:  2  loss:  462.1147461095825\n",
      "epoch:  3  loss:  452.64680490270257\n",
      "epoch:  4  loss:  444.89214462786913\n",
      "epoch:  5  loss:  436.7896318421699\n",
      "epoch:  6  loss:  430.099285031436\n",
      "epoch:  7  loss:  425.2490164996125\n",
      "epoch:  8  loss:  421.2367142714211\n",
      "epoch:  9  loss:  416.76439364926773\n",
      "epoch:  10  loss:  413.712205515214\n",
      "epoch:  11  loss:  410.12097829481354\n",
      "epoch:  12  loss:  406.7480441683074\n",
      "epoch:  13  loss:  403.4071175185745\n",
      "epoch:  14  loss:  399.6728444245964\n",
      "epoch:  15  loss:  395.5558434764698\n",
      "epoch:  16  loss:  389.3890879877731\n",
      "epoch:  17  loss:  386.92169112192096\n",
      "epoch:  18  loss:  381.5432640342069\n",
      "epoch:  19  loss:  374.8857741712077\n",
      "epoch:  20  loss:  373.188395698711\n",
      "epoch:  21  loss:  366.3218131561066\n",
      "epoch:  22  loss:  362.66298918341823\n",
      "epoch:  23  loss:  358.14281021215857\n",
      "epoch:  24  loss:  354.2800417264648\n",
      "epoch:  25  loss:  347.10938895953245\n",
      "epoch:  26  loss:  345.1015045643212\n",
      "epoch:  27  loss:  338.4268168780096\n",
      "epoch:  28  loss:  336.19407409892096\n",
      "epoch:  29  loss:  329.96038126664865\n",
      "epoch:  30  loss:  324.5489653479932\n",
      "epoch:  31  loss:  322.2063741297112\n",
      "epoch:  32  loss:  318.7699858809766\n",
      "epoch:  33  loss:  313.1184252299206\n",
      "epoch:  34  loss:  307.5666178262354\n",
      "epoch:  35  loss:  304.8394660620113\n",
      "epoch:  36  loss:  298.4804978442433\n",
      "epoch:  37  loss:  292.67468212446374\n",
      "epoch:  38  loss:  286.09434976648265\n",
      "epoch:  39  loss:  280.7415073944344\n",
      "epoch:  40  loss:  276.8238341171866\n",
      "epoch:  41  loss:  273.6706626867055\n",
      "epoch:  42  loss:  270.1616155615124\n",
      "epoch:  43  loss:  263.982889271169\n",
      "epoch:  44  loss:  264.2790971030104\n",
      "epoch:  45  loss:  257.65717025573537\n",
      "epoch:  46  loss:  251.7538267808922\n",
      "epoch:  47  loss:  247.7883963469742\n",
      "epoch:  48  loss:  245.3108556568115\n",
      "epoch:  49  loss:  240.21441864580422\n",
      "epoch:  50  loss:  239.1780110989826\n",
      "epoch:  51  loss:  235.1588397208063\n",
      "epoch:  52  loss:  230.5529111440904\n",
      "epoch:  53  loss:  231.53783123212224\n",
      "epoch:  54  loss:  224.11457268985083\n",
      "epoch:  55  loss:  222.47769905015332\n",
      "epoch:  56  loss:  219.6008045639604\n",
      "epoch:  57  loss:  217.77038721412765\n",
      "epoch:  58  loss:  213.36300382816114\n",
      "epoch:  59  loss:  217.26736115563554\n",
      "epoch:  60  loss:  213.0676109966821\n",
      "epoch:  61  loss:  211.00095162603645\n",
      "epoch:  62  loss:  203.0883902920679\n",
      "epoch:  63  loss:  202.04914968904387\n",
      "epoch:  64  loss:  202.94842354575763\n",
      "epoch:  65  loss:  206.92196173505351\n",
      "epoch:  66  loss:  202.92219572452723\n",
      "epoch:  67  loss:  208.1062980265951\n",
      "epoch:  68  loss:  195.66154973161093\n",
      "epoch:  69  loss:  192.86699698288805\n",
      "epoch:  70  loss:  191.03507733742723\n",
      "epoch:  71  loss:  184.26908438692521\n",
      "epoch:  72  loss:  188.73872426957607\n",
      "epoch:  73  loss:  176.19482862053724\n",
      "epoch:  74  loss:  182.9525085471915\n",
      "epoch:  75  loss:  174.6557736228056\n",
      "epoch:  76  loss:  167.51386504332265\n",
      "epoch:  77  loss:  167.59841957420878\n",
      "epoch:  78  loss:  171.81771980583696\n",
      "epoch:  79  loss:  170.3234908913152\n",
      "epoch:  80  loss:  152.0520635427433\n",
      "epoch:  81  loss:  152.01639126785318\n",
      "epoch:  82  loss:  150.57907145053292\n",
      "epoch:  83  loss:  145.11977382189804\n",
      "epoch:  84  loss:  146.15012909028158\n",
      "epoch:  85  loss:  147.50139921674082\n",
      "epoch:  86  loss:  142.16964452120567\n",
      "epoch:  87  loss:  136.76424529787496\n",
      "epoch:  88  loss:  131.3932321907738\n",
      "epoch:  89  loss:  134.42025961926458\n",
      "epoch:  90  loss:  142.54498375637098\n",
      "epoch:  91  loss:  128.31726748639787\n",
      "epoch:  92  loss:  122.71493358308715\n",
      "epoch:  93  loss:  123.61805488975506\n",
      "epoch:  94  loss:  127.85654946006984\n",
      "epoch:  95  loss:  121.23704659393763\n",
      "epoch:  96  loss:  115.96406340025865\n",
      "epoch:  97  loss:  111.12547085034208\n",
      "epoch:  98  loss:  103.41352322342036\n",
      "epoch:  99  loss:  104.84044517313029\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "model2 = copy.deepcopy(model1)\n",
    "criterion2 = nn.BCELoss()\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.0001)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model2.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(780):\n",
    "        optimizer2.zero_grad()\n",
    "        input = X_train_tensor[i+50]\n",
    "        label = y_train_tensor[i+50]\n",
    "        params = model2(input)\n",
    "        pred_result = generation(params).squeeze(-1)\n",
    "        loss = criterion2(pred_result, label)\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(\"epoch: \", epoch, \" loss: \", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 187\n",
      "Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "model1.eval()\n",
    "\n",
    "correct_count = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(280):\n",
    "        input = X_test_tensor[i]\n",
    "        label = y_test_tensor[i]\n",
    "        params = model1(input)\n",
    "        pred_result = generation(params).squeeze(-1)\n",
    "        pred = (pred_result > 0.5).float()\n",
    "        if pred == label:\n",
    "            correct_count += 1\n",
    "        total += 1\n",
    "print(f'Correct predictions: {correct_count}')\n",
    "print(f'Accuracy: {correct_count / total:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 196\n",
      "Accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "model2.eval()\n",
    "\n",
    "correct_count = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(280):\n",
    "        input = X_test_tensor[i]\n",
    "        label = y_test_tensor[i]\n",
    "        params = model2(input)\n",
    "        pred_result = generation(params).squeeze(-1)\n",
    "        pred = (pred_result > 0.5).float()\n",
    "        if pred == label:\n",
    "            correct_count += 1\n",
    "        \n",
    "        total += 1\n",
    "\n",
    "print(f'Correct predictions: {correct_count}')\n",
    "print(f'Accuracy: {correct_count / total:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 187\n",
      "Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "model2.eval()\n",
    "\n",
    "correct_count = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(280):\n",
    "        input = X_val_tensor[i]\n",
    "        label = y_val_tensor[i]\n",
    "        params = model2(input)\n",
    "        pred_result = generation(params).squeeze(-1)\n",
    "        pred = (pred_result > 0.5).float()\n",
    "        if pred == label:\n",
    "            correct_count += 1\n",
    "        total += 1\n",
    "\n",
    "print(f'Correct predictions: {correct_count}')\n",
    "print(f'Accuracy: {correct_count / total:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
